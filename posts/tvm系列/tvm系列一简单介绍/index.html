<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>TVM系列（一）简单介绍 | Patrick Blog</title>
<meta name=keywords content><meta name=description content='
主要记录一下使用TVM的一般方法

背景简介

上面是TVM的主要工作流程

从主流框架导入模型，例如Tensorflow, PyTorch, onnx等
将其转为Relay，Relay是TVM的上层模型语言，是针对神经网络设计的函数语言和IR，其主要具有的特征

支持传统数据流形式的表达
Functional-style scoping
允许用户混合上述两种编程风格


转为Tensor Expression（TE），TE用纯函数式语言描述张量计算
使用auto-tuning module来搜索最好的schedule，shedule指定了Tensor Expression中对于一个算子或子图的底层循环优化，TVM支持两种auto-tuning module

AutoTVM，基于模板的auto-tuning module，TVM提供了一系列模板
AutoScheduler（例如Ansor），无需模板的auto-tuning module，不需要预先定义的schedule模板，自动生成搜索空间


选择模型编译的最优配置，将tuning记录记录在json中，在这一步选择每个子图的最优schedule
将Tensor Expression（TE）转为Tensor Intermediate Representation（TIR），TVM的底层IR，使用底层优化pass优化TIR，然后将最终的代码放到可部署的后端上，主要包括以下的后端

LLVM
NVCC, NVIDIA’s compiler
嵌入式和特殊平台


生成机器码

简单示例
这里以super_resolution为例，介绍一下tvm的导入onnx模型，并编译的过程，该网络结构简单，适合用来研究TVM的工作原理

准备模型
首先下载onnx模型，这里记录两种下载onnx模型的方法

通过download_testdata方法（会下载到~/.tvm_test_data目录下）
在github上的https://github.com/onnx/models有许多现成的onnx模型

from tvm.contrib.download import download_testdata

model_url = "".join(
    [
        "<https://gist.github.com/zhreshold/>",
        "bcda4716699ac97ea44f791c24310193/raw/",
        "93672b029103648953c4e5ad3ac3aadf346a4cdc/",
        "super_resolution_0.2.onnx",
    ]
)
model_path = download_testdata(
    model_url, "super_resolution.onnx", module="onnx")
编译模型
最终会生成一个 .so 文件
import onnx
import tvm
import tvm.relay as relay

model_path = "./super_resolution.onnx"
onnx_model = onnx.load(model_path)

shape_dict = {"1": (1, 1, 224, 224)}
print(shape_dict)
mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)

with tvm.transform.PassContext(opt_level=1):
    lib = relay.build(mod, target="llvm", params=params)

lib.export_library("./super_resolution.so")
进行推理
使用其进行推理的过程也比较简单，下面的代码加载了上面编译得到的模型文件，并推理得到结果，比较核心的代码是中间那一段，最后放一个超分后的小猫图'><meta name=author content="patrickhao"><link rel=canonical href=https://patrickhao.github.io/posts/tvm%E7%B3%BB%E5%88%97/tvm%E7%B3%BB%E5%88%97%E4%B8%80%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://patrickhao.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://patrickhao.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://patrickhao.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://patrickhao.github.io/apple-touch-icon.png><link rel=mask-icon href=https://patrickhao.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://patrickhao.github.io/posts/tvm%E7%B3%BB%E5%88%97/tvm%E7%B3%BB%E5%88%97%E4%B8%80%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://patrickhao.github.io/posts/tvm%E7%B3%BB%E5%88%97/tvm%E7%B3%BB%E5%88%97%E4%B8%80%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"><meta property="og:site_name" content="Patrick Blog"><meta property="og:title" content="TVM系列（一）简单介绍"><meta property="og:description" content=' 主要记录一下使用TVM的一般方法
背景简介 上面是TVM的主要工作流程
从主流框架导入模型，例如Tensorflow, PyTorch, onnx等 将其转为Relay，Relay是TVM的上层模型语言，是针对神经网络设计的函数语言和IR，其主要具有的特征 支持传统数据流形式的表达 Functional-style scoping 允许用户混合上述两种编程风格 转为Tensor Expression（TE），TE用纯函数式语言描述张量计算 使用auto-tuning module来搜索最好的schedule，shedule指定了Tensor Expression中对于一个算子或子图的底层循环优化，TVM支持两种auto-tuning module AutoTVM，基于模板的auto-tuning module，TVM提供了一系列模板 AutoScheduler（例如Ansor），无需模板的auto-tuning module，不需要预先定义的schedule模板，自动生成搜索空间 选择模型编译的最优配置，将tuning记录记录在json中，在这一步选择每个子图的最优schedule 将Tensor Expression（TE）转为Tensor Intermediate Representation（TIR），TVM的底层IR，使用底层优化pass优化TIR，然后将最终的代码放到可部署的后端上，主要包括以下的后端 LLVM NVCC, NVIDIA’s compiler 嵌入式和特殊平台 生成机器码 简单示例 这里以super_resolution为例，介绍一下tvm的导入onnx模型，并编译的过程，该网络结构简单，适合用来研究TVM的工作原理 准备模型 首先下载onnx模型，这里记录两种下载onnx模型的方法
通过download_testdata方法（会下载到~/.tvm_test_data目录下） 在github上的https://github.com/onnx/models有许多现成的onnx模型 from tvm.contrib.download import download_testdata model_url = "".join( [ "<https://gist.github.com/zhreshold/>", "bcda4716699ac97ea44f791c24310193/raw/", "93672b029103648953c4e5ad3ac3aadf346a4cdc/", "super_resolution_0.2.onnx", ] ) model_path = download_testdata( model_url, "super_resolution.onnx", module="onnx") 编译模型 最终会生成一个 .so 文件
import onnx import tvm import tvm.relay as relay model_path = "./super_resolution.onnx" onnx_model = onnx.load(model_path) shape_dict = {"1": (1, 1, 224, 224)} print(shape_dict) mod, params = relay.frontend.from_onnx(onnx_model, shape_dict) with tvm.transform.PassContext(opt_level=1): lib = relay.build(mod, target="llvm", params=params) lib.export_library("./super_resolution.so") 进行推理 使用其进行推理的过程也比较简单，下面的代码加载了上面编译得到的模型文件，并推理得到结果，比较核心的代码是中间那一段，最后放一个超分后的小猫图'><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-03T15:11:51+08:00"><meta property="article:modified_time" content="2024-12-03T15:11:51+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="TVM系列（一）简单介绍"><meta name=twitter:description content='
主要记录一下使用TVM的一般方法

背景简介

上面是TVM的主要工作流程

从主流框架导入模型，例如Tensorflow, PyTorch, onnx等
将其转为Relay，Relay是TVM的上层模型语言，是针对神经网络设计的函数语言和IR，其主要具有的特征

支持传统数据流形式的表达
Functional-style scoping
允许用户混合上述两种编程风格


转为Tensor Expression（TE），TE用纯函数式语言描述张量计算
使用auto-tuning module来搜索最好的schedule，shedule指定了Tensor Expression中对于一个算子或子图的底层循环优化，TVM支持两种auto-tuning module

AutoTVM，基于模板的auto-tuning module，TVM提供了一系列模板
AutoScheduler（例如Ansor），无需模板的auto-tuning module，不需要预先定义的schedule模板，自动生成搜索空间


选择模型编译的最优配置，将tuning记录记录在json中，在这一步选择每个子图的最优schedule
将Tensor Expression（TE）转为Tensor Intermediate Representation（TIR），TVM的底层IR，使用底层优化pass优化TIR，然后将最终的代码放到可部署的后端上，主要包括以下的后端

LLVM
NVCC, NVIDIA’s compiler
嵌入式和特殊平台


生成机器码

简单示例
这里以super_resolution为例，介绍一下tvm的导入onnx模型，并编译的过程，该网络结构简单，适合用来研究TVM的工作原理

准备模型
首先下载onnx模型，这里记录两种下载onnx模型的方法

通过download_testdata方法（会下载到~/.tvm_test_data目录下）
在github上的https://github.com/onnx/models有许多现成的onnx模型

from tvm.contrib.download import download_testdata

model_url = "".join(
    [
        "<https://gist.github.com/zhreshold/>",
        "bcda4716699ac97ea44f791c24310193/raw/",
        "93672b029103648953c4e5ad3ac3aadf346a4cdc/",
        "super_resolution_0.2.onnx",
    ]
)
model_path = download_testdata(
    model_url, "super_resolution.onnx", module="onnx")
编译模型
最终会生成一个 .so 文件
import onnx
import tvm
import tvm.relay as relay

model_path = "./super_resolution.onnx"
onnx_model = onnx.load(model_path)

shape_dict = {"1": (1, 1, 224, 224)}
print(shape_dict)
mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)

with tvm.transform.PassContext(opt_level=1):
    lib = relay.build(mod, target="llvm", params=params)

lib.export_library("./super_resolution.so")
进行推理
使用其进行推理的过程也比较简单，下面的代码加载了上面编译得到的模型文件，并推理得到结果，比较核心的代码是中间那一段，最后放一个超分后的小猫图'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://patrickhao.github.io/posts/"},{"@type":"ListItem","position":2,"name":"TVM系列（一）简单介绍","item":"https://patrickhao.github.io/posts/tvm%E7%B3%BB%E5%88%97/tvm%E7%B3%BB%E5%88%97%E4%B8%80%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"TVM系列（一）简单介绍","name":"TVM系列（一）简单介绍","description":" 主要记录一下使用TVM的一般方法\n背景简介 上面是TVM的主要工作流程\n从主流框架导入模型，例如Tensorflow, PyTorch, onnx等 将其转为Relay，Relay是TVM的上层模型语言，是针对神经网络设计的函数语言和IR，其主要具有的特征 支持传统数据流形式的表达 Functional-style scoping 允许用户混合上述两种编程风格 转为Tensor Expression（TE），TE用纯函数式语言描述张量计算 使用auto-tuning module来搜索最好的schedule，shedule指定了Tensor Expression中对于一个算子或子图的底层循环优化，TVM支持两种auto-tuning module AutoTVM，基于模板的auto-tuning module，TVM提供了一系列模板 AutoScheduler（例如Ansor），无需模板的auto-tuning module，不需要预先定义的schedule模板，自动生成搜索空间 选择模型编译的最优配置，将tuning记录记录在json中，在这一步选择每个子图的最优schedule 将Tensor Expression（TE）转为Tensor Intermediate Representation（TIR），TVM的底层IR，使用底层优化pass优化TIR，然后将最终的代码放到可部署的后端上，主要包括以下的后端 LLVM NVCC, NVIDIA’s compiler 嵌入式和特殊平台 生成机器码 简单示例 这里以super_resolution为例，介绍一下tvm的导入onnx模型，并编译的过程，该网络结构简单，适合用来研究TVM的工作原理 准备模型 首先下载onnx模型，这里记录两种下载onnx模型的方法\n通过download_testdata方法（会下载到~/.tvm_test_data目录下） 在github上的https://github.com/onnx/models有许多现成的onnx模型 from tvm.contrib.download import download_testdata model_url = \u0026#34;\u0026#34;.join( [ \u0026#34;\u0026lt;https://gist.github.com/zhreshold/\u0026gt;\u0026#34;, \u0026#34;bcda4716699ac97ea44f791c24310193/raw/\u0026#34;, \u0026#34;93672b029103648953c4e5ad3ac3aadf346a4cdc/\u0026#34;, \u0026#34;super_resolution_0.2.onnx\u0026#34;, ] ) model_path = download_testdata( model_url, \u0026#34;super_resolution.onnx\u0026#34;, module=\u0026#34;onnx\u0026#34;) 编译模型 最终会生成一个 .so 文件\nimport onnx import tvm import tvm.relay as relay model_path = \u0026#34;./super_resolution.onnx\u0026#34; onnx_model = onnx.load(model_path) shape_dict = {\u0026#34;1\u0026#34;: (1, 1, 224, 224)} print(shape_dict) mod, params = relay.frontend.from_onnx(onnx_model, shape_dict) with tvm.transform.PassContext(opt_level=1): lib = relay.build(mod, target=\u0026#34;llvm\u0026#34;, params=params) lib.export_library(\u0026#34;./super_resolution.so\u0026#34;) 进行推理 使用其进行推理的过程也比较简单，下面的代码加载了上面编译得到的模型文件，并推理得到结果，比较核心的代码是中间那一段，最后放一个超分后的小猫图\n","keywords":[],"articleBody":" 主要记录一下使用TVM的一般方法\n背景简介 上面是TVM的主要工作流程\n从主流框架导入模型，例如Tensorflow, PyTorch, onnx等 将其转为Relay，Relay是TVM的上层模型语言，是针对神经网络设计的函数语言和IR，其主要具有的特征 支持传统数据流形式的表达 Functional-style scoping 允许用户混合上述两种编程风格 转为Tensor Expression（TE），TE用纯函数式语言描述张量计算 使用auto-tuning module来搜索最好的schedule，shedule指定了Tensor Expression中对于一个算子或子图的底层循环优化，TVM支持两种auto-tuning module AutoTVM，基于模板的auto-tuning module，TVM提供了一系列模板 AutoScheduler（例如Ansor），无需模板的auto-tuning module，不需要预先定义的schedule模板，自动生成搜索空间 选择模型编译的最优配置，将tuning记录记录在json中，在这一步选择每个子图的最优schedule 将Tensor Expression（TE）转为Tensor Intermediate Representation（TIR），TVM的底层IR，使用底层优化pass优化TIR，然后将最终的代码放到可部署的后端上，主要包括以下的后端 LLVM NVCC, NVIDIA’s compiler 嵌入式和特殊平台 生成机器码 简单示例 这里以super_resolution为例，介绍一下tvm的导入onnx模型，并编译的过程，该网络结构简单，适合用来研究TVM的工作原理 准备模型 首先下载onnx模型，这里记录两种下载onnx模型的方法\n通过download_testdata方法（会下载到~/.tvm_test_data目录下） 在github上的https://github.com/onnx/models有许多现成的onnx模型 from tvm.contrib.download import download_testdata model_url = \"\".join( [ \"\", \"bcda4716699ac97ea44f791c24310193/raw/\", \"93672b029103648953c4e5ad3ac3aadf346a4cdc/\", \"super_resolution_0.2.onnx\", ] ) model_path = download_testdata( model_url, \"super_resolution.onnx\", module=\"onnx\") 编译模型 最终会生成一个 .so 文件\nimport onnx import tvm import tvm.relay as relay model_path = \"./super_resolution.onnx\" onnx_model = onnx.load(model_path) shape_dict = {\"1\": (1, 1, 224, 224)} print(shape_dict) mod, params = relay.frontend.from_onnx(onnx_model, shape_dict) with tvm.transform.PassContext(opt_level=1): lib = relay.build(mod, target=\"llvm\", params=params) lib.export_library(\"./super_resolution.so\") 进行推理 使用其进行推理的过程也比较简单，下面的代码加载了上面编译得到的模型文件，并推理得到结果，比较核心的代码是中间那一段，最后放一个超分后的小猫图\nfrom matplotlib import pyplot as plt from PIL import Image import tvm from tvm.contrib import graph_executor import numpy as np # 准备输入数据 img = Image.open(\"./cat.png\").resize((224, 224)) img_ycbcr = img.convert(\"YCbCr\") # convert to YCbCr img_y, img_cb, img_cr = img_ycbcr.split() x = np.array(img_y)[np.newaxis, np.newaxis, :, :] # 下面这一段加载模型并得到推理结果 lib = tvm.runtime.load_module(\"./super_resolution.so\") dev = tvm.cpu(0) gmod = graph_executor.GraphModule(lib[\"default\"](dev)) gmod.set_input(\"1\", tvm.nd.array(x.astype(\"float32\"))) gmod.run() tvm_output = gmod.get_output(0).numpy() # 输出数据可视化 out_y = Image.fromarray(np.uint8((tvm_output[0, 0]).clip(0, 255)), mode=\"L\") out_cb = img_cb.resize(out_y.size, Image.BICUBIC) out_cr = img_cr.resize(out_y.size, Image.BICUBIC) result = Image.merge(\"YCbCr\", [out_y, out_cb, out_cr]).convert(\"RGB\") canvas = np.full((672, 672 * 2, 3), 255) canvas[0:224, 0:224, :] = np.asarray(img) canvas[:, 672:, :] = np.asarray(result) plt.imshow(canvas.astype(np.uint8)) plt.show() ","wordCount":"202","inLanguage":"en","datePublished":"2024-12-03T15:11:51+08:00","dateModified":"2024-12-03T15:11:51+08:00","author":{"@type":"Person","name":"patrickhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://patrickhao.github.io/posts/tvm%E7%B3%BB%E5%88%97/tvm%E7%B3%BB%E5%88%97%E4%B8%80%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},"publisher":{"@type":"Organization","name":"Patrick Blog","logo":{"@type":"ImageObject","url":"https://patrickhao.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://patrickhao.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://patrickhao.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://patrickhao.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://patrickhao.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://patrickhao.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://patrickhao.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">TVM系列（一）简单介绍</h1><div class=post-meta><span title='2024-12-03 15:11:51 +0800 CST'>December 3, 2024</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;202 words&nbsp;·&nbsp;patrickhao</div></header><div class=post-content><blockquote><p>主要记录一下使用TVM的一般方法</p></blockquote><h1 id=背景简介>背景简介<a hidden class=anchor aria-hidden=true href=#背景简介>#</a></h1><p><img alt=Pasted_image_20241203130731.png loading=lazy src=https://patrickhao.github.io/Pasted_image_20241203130731.png>
上面是TVM的主要工作流程</p><ol><li>从主流框架导入模型，例如Tensorflow, PyTorch, onnx等</li><li>将其转为Relay，Relay是TVM的上层模型语言，是针对神经网络设计的函数语言和IR，其主要具有的特征<ul><li>支持传统数据流形式的表达</li><li>Functional-style scoping</li><li>允许用户混合上述两种编程风格</li></ul></li><li>转为Tensor Expression（TE），TE用纯函数式语言描述张量计算</li><li>使用auto-tuning module来搜索最好的schedule，shedule指定了Tensor Expression中对于一个算子或子图的底层循环优化，TVM支持两种auto-tuning module<ul><li>AutoTVM，基于模板的auto-tuning module，TVM提供了一系列模板</li><li>AutoScheduler（例如Ansor），无需模板的auto-tuning module，不需要预先定义的schedule模板，自动生成搜索空间</li></ul></li><li>选择模型编译的最优配置，将tuning记录记录在json中，在这一步选择每个子图的最优schedule</li><li>将Tensor Expression（TE）转为Tensor Intermediate Representation（TIR），TVM的底层IR，使用底层优化pass优化TIR，然后将最终的代码放到可部署的后端上，主要包括以下的后端<ul><li>LLVM</li><li>NVCC, NVIDIA’s compiler</li><li>嵌入式和特殊平台</li></ul></li><li>生成机器码</li></ol><h1 id=简单示例>简单示例<a hidden class=anchor aria-hidden=true href=#简单示例>#</a></h1><p>这里以super_resolution为例，介绍一下tvm的导入onnx模型，并编译的过程，该网络结构简单，适合用来研究TVM的工作原理
<img alt=Pasted_image_20241203130817.png loading=lazy src=https://patrickhao.github.io/Pasted_image_20241203130817.png></p><h2 id=准备模型>准备模型<a hidden class=anchor aria-hidden=true href=#准备模型>#</a></h2><p>首先下载onnx模型，这里记录两种下载onnx模型的方法</p><ul><li>通过download_testdata方法（会下载到~/.tvm_test_data目录下）</li><li>在github上的https://github.com/onnx/models有许多现成的onnx模型</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tvm.contrib.download</span> <span class=kn>import</span> <span class=n>download_testdata</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_url</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&lt;https://gist.github.com/zhreshold/&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;bcda4716699ac97ea44f791c24310193/raw/&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;93672b029103648953c4e5ad3ac3aadf346a4cdc/&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;super_resolution_0.2.onnx&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=n>download_testdata</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_url</span><span class=p>,</span> <span class=s2>&#34;super_resolution.onnx&#34;</span><span class=p>,</span> <span class=n>module</span><span class=o>=</span><span class=s2>&#34;onnx&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=编译模型>编译模型<a hidden class=anchor aria-hidden=true href=#编译模型>#</a></h2><p>最终会生成一个 <code>.so</code> 文件</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>import</span> <span class=n>onnx</span>
</span></span><span class=line><span class=cl><span class=n>import</span> <span class=n>tvm</span>
</span></span><span class=line><span class=cl><span class=n>import</span> <span class=n>tvm</span><span class=p>.</span><span class=n>relay</span> <span class=n>as</span> <span class=n>relay</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=s>&#34;./super_resolution.onnx&#34;</span>
</span></span><span class=line><span class=cl><span class=n>onnx_model</span> <span class=o>=</span> <span class=n>onnx</span><span class=p>.</span><span class=n>load</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>shape_dict</span> <span class=o>=</span> <span class=p>{</span><span class=s>&#34;1&#34;</span><span class=o>:</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>)}</span>
</span></span><span class=line><span class=cl><span class=n>print</span><span class=p>(</span><span class=n>shape_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mod</span><span class=p>,</span> <span class=n>params</span> <span class=o>=</span> <span class=n>relay</span><span class=p>.</span><span class=n>frontend</span><span class=p>.</span><span class=n>from_onnx</span><span class=p>(</span><span class=n>onnx_model</span><span class=p>,</span> <span class=n>shape_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>with</span> <span class=n>tvm</span><span class=p>.</span><span class=n>transform</span><span class=p>.</span><span class=n>PassContext</span><span class=p>(</span><span class=n>opt_level</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>:</span>
</span></span><span class=line><span class=cl>    <span class=n>lib</span> <span class=o>=</span> <span class=n>relay</span><span class=p>.</span><span class=n>build</span><span class=p>(</span><span class=n>mod</span><span class=p>,</span> <span class=n>target</span><span class=o>=</span><span class=s>&#34;llvm&#34;</span><span class=p>,</span> <span class=n>params</span><span class=o>=</span><span class=n>params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>lib</span><span class=p>.</span><span class=n>export_library</span><span class=p>(</span><span class=s>&#34;./super_resolution.so&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=进行推理>进行推理<a hidden class=anchor aria-hidden=true href=#进行推理>#</a></h2><p>使用其进行推理的过程也比较简单，下面的代码加载了上面编译得到的模型文件，并推理得到结果，比较核心的代码是中间那一段，最后放一个超分后的小猫图</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>matplotlib</span> <span class=kn>import</span> <span class=n>pyplot</span> <span class=k>as</span> <span class=n>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tvm</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tvm.contrib</span> <span class=kn>import</span> <span class=n>graph_executor</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 准备输入数据</span>
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s2>&#34;./cat.png&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>resize</span><span class=p>((</span><span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>img_ycbcr</span> <span class=o>=</span> <span class=n>img</span><span class=o>.</span><span class=n>convert</span><span class=p>(</span><span class=s2>&#34;YCbCr&#34;</span><span class=p>)</span>  <span class=c1># convert to YCbCr</span>
</span></span><span class=line><span class=cl><span class=n>img_y</span><span class=p>,</span> <span class=n>img_cb</span><span class=p>,</span> <span class=n>img_cr</span> <span class=o>=</span> <span class=n>img_ycbcr</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>img_y</span><span class=p>)[</span><span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 下面这一段加载模型并得到推理结果</span>
</span></span><span class=line><span class=cl><span class=n>lib</span> <span class=o>=</span> <span class=n>tvm</span><span class=o>.</span><span class=n>runtime</span><span class=o>.</span><span class=n>load_module</span><span class=p>(</span><span class=s2>&#34;./super_resolution.so&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dev</span> <span class=o>=</span> <span class=n>tvm</span><span class=o>.</span><span class=n>cpu</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gmod</span> <span class=o>=</span> <span class=n>graph_executor</span><span class=o>.</span><span class=n>GraphModule</span><span class=p>(</span><span class=n>lib</span><span class=p>[</span><span class=s2>&#34;default&#34;</span><span class=p>](</span><span class=n>dev</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>gmod</span><span class=o>.</span><span class=n>set_input</span><span class=p>(</span><span class=s2>&#34;1&#34;</span><span class=p>,</span> <span class=n>tvm</span><span class=o>.</span><span class=n>nd</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s2>&#34;float32&#34;</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>gmod</span><span class=o>.</span><span class=n>run</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tvm_output</span> <span class=o>=</span> <span class=n>gmod</span><span class=o>.</span><span class=n>get_output</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 输出数据可视化</span>
</span></span><span class=line><span class=cl><span class=n>out_y</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>fromarray</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>uint8</span><span class=p>((</span><span class=n>tvm_output</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>255</span><span class=p>)),</span> <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;L&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_cb</span> <span class=o>=</span> <span class=n>img_cb</span><span class=o>.</span><span class=n>resize</span><span class=p>(</span><span class=n>out_y</span><span class=o>.</span><span class=n>size</span><span class=p>,</span> <span class=n>Image</span><span class=o>.</span><span class=n>BICUBIC</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_cr</span> <span class=o>=</span> <span class=n>img_cr</span><span class=o>.</span><span class=n>resize</span><span class=p>(</span><span class=n>out_y</span><span class=o>.</span><span class=n>size</span><span class=p>,</span> <span class=n>Image</span><span class=o>.</span><span class=n>BICUBIC</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=s2>&#34;YCbCr&#34;</span><span class=p>,</span> <span class=p>[</span><span class=n>out_y</span><span class=p>,</span> <span class=n>out_cb</span><span class=p>,</span> <span class=n>out_cr</span><span class=p>])</span><span class=o>.</span><span class=n>convert</span><span class=p>(</span><span class=s2>&#34;RGB&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>canvas</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>full</span><span class=p>((</span><span class=mi>672</span><span class=p>,</span> <span class=mi>672</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=mi>255</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>canvas</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>224</span><span class=p>,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>224</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>canvas</span><span class=p>[:,</span> <span class=mi>672</span><span class=p>:,</span> <span class=p>:]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>canvas</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>uint8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p><img alt=Pasted_image_20241203130836.png loading=lazy src=https://patrickhao.github.io/Pasted_image_20241203130836.png></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://patrickhao.github.io/posts/mini%E7%B3%BB%E5%88%97/mini%E7%B3%BB%E5%88%97%E4%B8%80minigrep/><span class=title>« Prev</span><br><span>mini系列（一）minigrep</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://patrickhao.github.io/>Patrick Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>